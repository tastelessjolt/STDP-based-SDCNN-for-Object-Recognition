\documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{STDP-based Spiking Deep Convolution Neural Networks for Object Recognition}

\author{\IEEEauthorblockN{Harshith Goka}
\IEEEauthorblockA{\textit{Department of Computer Science and Engineering} \\
\textit{Indian Institute of Technology Bombay}\\
harshith9399@gmail.com}
\and
\IEEEauthorblockN{Chinmaya Kumar Sahoo}
\IEEEauthorblockA{\textit{Department of Electrical Engineering} \\
\textit{Indian Institute of Technology Bombay}\\
chinmayas800@gmail.com}
}

\maketitle

\begin{abstract}
Authors proposed an STDP-based Spiking Deep Neural Network(SDNN) comprising several convolutional (trainable with STDP) and pooling layers with a temporal coding scheme where the most strongly activated neurons fire first, and less activated neurons fire later or not at all. It is inspired by Deep Convolutional Networks where many layers increase the robustness of image recognition task. We experimented with some extensions and changes to the original work.
\end{abstract}

\begin{IEEEkeywords}
Spiking Neural Network, STDP, Deep Learning, Temporal Coding, Convolution Neural Network, Object Recognition
\end{IEEEkeywords}

\section{Introduction}
Original paper\cite{orig} by Kheradpisheh et al. explored Deep Spiking Neural Network which added robustness to image recognition task by making the network deep. They proposed an STDP-based spiking deep neural network (SDNN) with a spike-time neural coding. The network is comprised of a temporal-coding layer followed by a cascade of consecutive convolutional (feature extractor) and pooling layers. The first layer converts the input image into an asynchronous spike train, where the visual information is encoded in the temporal order of the spikes. Neurons in convolutional layers integrate input spikes and emit a spike right after reaching their threshold. These layers are equipped with STDP to learn visual features. Pooling layers provide translation invariance and also compact the visual information. Through the network, visual features get larger and more complex, where neurons in the last convolutional layer learn and detect object prototypes. In the end, a classifier detects the category of the input image based on the activity of neurons in the last pooling layer with global receptive fields.


We experimented with new changes and extensions to the original work. We changed the simplified STDP learning rule to exponential STDP learning rule. We experimented by removing lateral inhibition as we felt implementation is not biologically plausible. We extended the original work to do recognition task on coloured images.\\

\section{Changes proposed to the original paper}

\subsection{Exponential STDP}
The original work \cite{orig} have used a time length independent STDP learning rule. They justify their update rule with the argument that the time frame for processing is small, hence no need to update the weights differentially. But we wanted to challenge this assumption and use a time-dependent STDP learning rule because it represents better the actual biological process. 
The exact formulation of the exponential STDP update rule is as follows,
$$ \Delta w_{ij} = 
\begin{cases}
    a^{+} w_{ij} (1 - w_{ij}) e ^ { \frac{-(t_i - t_j)}{\tau^{+} } } & \text{if } t_j - t_i \leq 0, \\
    a^{-} w_{ij} (1 - w_{ij}) e ^ { \frac{-(t_i - t_j)}{\tau^{-} } } & \text{if } t_j - t_i > 0 \\
\end{cases} $$

\subsection{Removing Lateral Inhibition}
In original work neuron at same position across different filters in the same convolutional were not allowed to spike forcefully. We experimented by removing this constraint.

\subsection{Using Coloured Images}
We extended the original work by using coloured images. We passed different channel information of image through DoG filter and temporal encoding.

\section{Experimental Setup}

\subsection{Architecture \& Dataset}
Going with the original work, we used the same architecture, three convolutional layers each followed by a pooling layer. For the first layer, only ON-center DoG filters of size $7 \times 7$ and standard deviations of 1 and 2 pixels are used. The first, second and third convolutional layers consists of 4, 20, and 20 neuronal maps with conv-window sizes $5 \times 5$, $ 17 \times 17 $ and $ 5 \times 5 $, respectively. The pooling window sizes of the first and second pooling layers are $ 7 \times 7 $ and $ 5 \times 5 $ with strides 6 and 5 respectively. The third pooling layer performs a global max pooling operation. \\
As with the original paper, we evaluated on the same dataset i.e., face and motorbike categories of Caltech 101 dataset available at \url{http://www.vision.caltech.edu}. The training set contains 200 randomly selected images per category, and remaining images constitute the test set. 

\subsection{Evaluation Metrics}
We compare the classification accuracy on the test set images which the network has never seen before. In all the experiments, we have used linear SVM classifiers with penalty parameter $C = 1.0$ (optimized by a grid search in the range of $(0, 10])$.

\subsection{Code}
We have used the implementation made public by the authors at \url{https://github.com/npvoid/SDNN\_python} and made the proposed changes to the code. 

\section{Results}
% \subsection{Baseline}
\subsection{Exponential STDP}
While implementing, we had to slow down the STDP update since previously, they were assuming that if a pre-neuron does not spike before the post-neuron, then it definitely will spike after the post-neuron. So, our training time nearly doubled. 

We have been tuning the hyperparameters for better performance on the test set, though the best we have yet using the exponential STDP rule is 78.5\%. But, when we visualised the convolutional weight maps, they still haven't converged to either 0 or 1. So, we think that there is a need for better hyperparameter tuning.

\subsection{Removing Lateral Inhibition}
For removing lateral inhibition we changed in the code snippet where the neuron in the same position across different filters in the same layer is not allowed to spike in the same episode. After tuning, we reached an accuracy of 81\%.

We think inhibition across layer is important as only one filter should learn a feature and at the same position only one filter learn by STDP. But the method of inhibition implementation should be different like - once a neuron in a filter spikes it should suppress spiking in other filters but by a mechanism which is biologically plausible. We are planning to explore.  

\subsection{Using Coloured Images}
We extended the architecture for object recognition task of colour images as well. We passed a 3-Dimensional array corresponding to different channels(RGB) and then encoded with temporal encoding. We changed the filters' dimension in the network to operate on volume of spikes which DoG layer is giving as output.

We trained and tested Caltech motorbike dataset with colour images. After some hyperparameter tuning, we have reached an accuracy of 75\% yet. We are hopeful about getting better performance after tuning. 

\section{Plan for Phase-II}
\subsection{Find Preferred Feature of Filters}
We plan to improve the results in Phase-I by understanding exactly what is going on with the model. So to this end, we will visualise preferred visual feature learned by a neuron using
backward reconstruction technique. The visual features in the current layer can be reconstructed as the weighted combinations of the visual features in the previous layer. This backward process continues until the first layer, whose preferred visual features are computed by DoG functions.

\subsection{Redundancy Removal from Coloured Images}
We observed that the RGB channels were sending similar information after being passed through the DoG layer. So we will send different channel information of the image by a different method.

\subsection{Use LIF Neuron model}
We plan to use a different Neuron model, and LIF is quite close to IF. And the leaky part might be an essential part since it is about the forgetting farther old pre-neuron spikes.

\subsection{Triangular STDP}
We will approximate the exponential STDP learning rule by triangle approximation for faster computation.



% \section{Remove this section}
% Before you begin to format your paper, first write and save the content as a 
% \subsection{Abbreviations and Acronyms}\label{AA}
% Define abbreviations and acronyms the first time they are used in the text, 
% even after they have been defined in the abstract. Abbreviations such as 
% IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
% abbreviations in the title or heads unless they are unavoidable.

% \subsection{Units}
% \begin{itemize}
% \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
% \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
% \item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
% \item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
% \end{itemize}

% \subsection{Equations}
% Number equations consecutively. To make your 
% equations more compact, you may use the solidus (~/~), the exp function, or 
% appropriate exponents. Italicize Roman symbols for quantities and variables, 
% but not Greek symbols. Use a long dash rather than a hyphen for a minus 
% sign. Punctuate equations with commas or periods when they are part of a 
% sentence, as in:
% \begin{equation}
% a+b=\gamma\label{eq}
% \end{equation}

% Be sure that the 
% symbols in your equation have been defined before or immediately following 
% the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
% the beginning of a sentence: ``Equation \eqref{eq} is . . .''

% \subsection{\LaTeX-Specific Advice}

% Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
% of ``hard'' references (e.g., \verb|(1)|). That will make it possible
% to combine sections, add equations, or change the order of figures or
% citations without having to go through the file line by line.

% Please don't use the \verb|{eqnarray}| equation environment. Use
% \verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
% environment leaves unsightly spaces around relation symbols.

% Please note that the \verb|{subequations}| environment in {\LaTeX}
% will increment the main equation counter even when there are no
% equation numbers displayed. If you forget that, you might write an
% article in which the equation numbers skip from (17) to (20), causing
% the copy editors to wonder if you've discovered a new method of
% counting.

% {\BibTeX} does not work by magic. It doesn't get the bibliographic
% data from thin air but from .bib files. If you use {\BibTeX} to produce a
% bibliography you must send the .bib files. 

% {\LaTeX} can't read your mind. If you assign the same label to a
% subsubsection and a table, you might find that Table I has been cross
% referenced as Table IV-B3. 

% {\LaTeX} does not have precognitive abilities. If you put a
% \verb|\label| command before the command that updates the counter it's
% supposed to be using, the label will pick up the last counter to be
% cross referenced instead. In particular, a \verb|\label| command
% should not go before the caption of a figure or a table.

% Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
% will not stop equation numbers inside \verb|{array}| (there won't be
% any anyway) and it might stop a wanted equation number in the
% surrounding equation.

% \subsection{Some Common Mistakes}\label{SCM}
% \begin{itemize}
% \item The word ``data'' is plural, not singular.
% \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
% \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
% \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
% \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
% \item Do not confuse ``imply'' and ``infer''.
% \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
% \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
% \end{itemize}
% An excellent style manual for science writers is \cite{b7}.

% \subsection{Authors and Affiliations}
% \textbf{The class file is designed for, but not limited to, six authors.} A 
% minimum of one author is required for all conference articles. Author names 
% should be listed starting from left to right and then moving down to the 
% next line. This is the author sequence that will be used in future citations 
% and by indexing services. Names should not be listed in columns nor group by 
% affiliation. Please keep your affiliations as succinct as possible (for 
% example, do not differentiate among departments of the same organization).

% \subsection{Identify the Headings}
% Headings, or heads, are organizational devices that guide the reader through 
% your paper. There are two types: component heads and text heads.

% Component heads identify the different components of your paper and are not 
% topically subordinate to each other. Examples include Acknowledgments and 
% References and, for these, the correct style to use is ``Heading 5''. Use 
% ``figure caption'' for your Figure captions, and ``table head'' for your 
% table title. Run-in heads, such as ``Abstract'', will require you to apply a 
% style (in this case, italic) in addition to the style provided by the drop 
% down menu to differentiate the head from the text.

% Text heads organize the topics on a relational, hierarchical basis. For 
% example, the paper title is the primary text head because all subsequent 
% material relates and elaborates on this one topic. If there are two or more 
% sub-topics, the next level head (uppercase Roman numerals) should be used 
% and, conversely, if there are not at least two sub-topics, then no subheads 
% should be introduced.

% \subsection{Figures and Tables}
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
% rather than symbols or abbreviations when writing Figure axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 
% ``Temperature/K''.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.

% \section*{References}

% Please number citations consecutively within brackets \cite{b1}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
\bibitem{orig} Kheradpisheh, S.R., Ganjtabesh, M., Thorpe, S.J., Masquelier, T., STDP-based spiking deep
convolutional neural networks for object recognition. Neural Networks (2017).
https: // doi. org/ 10. 1016/ j. neunet. 2017. 12. 005
\end{thebibliography}
\end{document}
